{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, l2_lambda = 0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.xavier_uniform_(self.hidden.weight)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        \n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the neural network.\n",
    "        \"\"\"\n",
    "        x = self.hidden(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.load_boston()['data']\n",
    "y = datasets.load_boston()['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = NeuralNetwork(input_size = X_train.shape[1], hidden_size = 50, output_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(self.parameters(), \n",
    "            lr=0.01, \n",
    "            momentum = 0.9, \n",
    "            weight_decay=0.01 #l2 reg\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.train()  # turn on dropout\n",
    "\n",
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = self(torch.Tensor(X_train))\n",
    "    loss = criterion(output, torch.Tensor(y_train).view(-1,1))\n",
    "\n",
    "    # Calculates gradient\n",
    "    loss.backward()\n",
    "    print(self.hidden.weight[0])\n",
    "\n",
    "    # Updates weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.eval()  # turn off dropout\n",
    "with torch.no_grad():  # turn off autograd for faster computation and to save memory\n",
    "    predicted_output = self(torch.Tensor(X_test))\n",
    "sum((predicted_output - torch.Tensor(y_test).view(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in self.bn.state_dict().items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class QuickProp(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr)\n",
    "        super(QuickProp, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_p = p.grad\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['prev_delta'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state['prev_update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                prev_delta = state['prev_delta']\n",
    "                prev_update = state['prev_update']\n",
    "\n",
    "                denom = prev_delta - d_p + 1e-10  # Add epsilon to prevent division by zero\n",
    "                update = d_p * prev_update / denom\n",
    "                p.add_(update, alpha=-group['lr'])\n",
    "\n",
    "                # Update state\n",
    "                state['prev_delta'] = d_p.clone()\n",
    "                state['prev_update'] = update.clone()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = QuickProp(self.parameters(), \n",
    "            lr=0.01, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.train()  # turn on dropout\n",
    "\n",
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = self(torch.Tensor(X_train))\n",
    "    loss = criterion(output, torch.Tensor(y_train).view(-1,1))\n",
    "\n",
    "    # Calculates gradient\n",
    "    loss.backward()\n",
    "    print(self.hidden.weight[0])\n",
    "\n",
    "    # Updates weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in optimizer.state.items():\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c2396ee008e4910c299ae7134fa9bf09084771bab5830620999247b4a514b46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
