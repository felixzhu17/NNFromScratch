{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network with one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, l2_lambda = 0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.xavier_uniform_(self.hidden.weight)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        \n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the neural network.\n",
    "        \"\"\"\n",
    "        x = self.hidden(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.load_boston()['data']\n",
    "y = datasets.load_boston()['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = NeuralNetwork(input_size = X_train.shape[1], hidden_size = 50, output_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(self.parameters(), \n",
    "            lr=0.01, \n",
    "            momentum = 0.9, \n",
    "            weight_decay=0.01 #l2 reg\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2589, -0.0437,  0.1635, -0.1984, -0.1975,  0.1917,  0.1334,  0.2452,\n",
      "        -0.0257, -0.1598, -0.1703, -0.0999,  0.2761], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2568, -0.0444,  0.1668, -0.1995, -0.1950,  0.1863,  0.1357,  0.2455,\n",
      "        -0.0231, -0.1568, -0.1674, -0.1021,  0.2822], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2573, -0.0425,  0.1648, -0.1992, -0.1964,  0.1879,  0.1342,  0.2469,\n",
      "        -0.0235, -0.1577, -0.1686, -0.1009,  0.2802], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2639, -0.0318,  0.1506, -0.1898, -0.2083,  0.2150,  0.1228,  0.2512,\n",
      "        -0.0325, -0.1693, -0.1855, -0.0894,  0.2528], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2710, -0.0213,  0.1315, -0.1693, -0.2232,  0.2702,  0.1115,  0.2490,\n",
      "        -0.0430, -0.1859, -0.2178, -0.0678,  0.2033], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2705, -0.0151,  0.1218, -0.1399, -0.2299,  0.3428,  0.1132,  0.2331,\n",
      "        -0.0418, -0.1949, -0.2592, -0.0492,  0.1535], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2590, -0.0125,  0.1210, -0.1132, -0.2328,  0.4246,  0.1213,  0.2120,\n",
      "        -0.0269, -0.1929, -0.2986, -0.0361,  0.1068], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2428, -0.0047,  0.1194, -0.0948, -0.2392,  0.5025,  0.1240,  0.1948,\n",
      "        -0.0093, -0.1900, -0.3312, -0.0205,  0.0627], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2273,  0.0029,  0.1135, -0.0820, -0.2511,  0.5723,  0.1207,  0.1820,\n",
      "         0.0050, -0.1905, -0.3572, -0.0018,  0.0195], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2136,  0.0105,  0.1049, -0.0738, -0.2673,  0.6325,  0.1141,  0.1730,\n",
      "         0.0151, -0.1947, -0.3791,  0.0190, -0.0217], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "self.train()  # turn on dropout\n",
    "\n",
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = self(torch.Tensor(X_train))\n",
    "    loss = criterion(output, torch.Tensor(y_train).view(-1,1))\n",
    "\n",
    "    # Calculates gradient\n",
    "    loss.backward()\n",
    "    print(self.hidden.weight[0])\n",
    "\n",
    "    # Updates weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10584.7480])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.eval()  # turn off dropout\n",
    "with torch.no_grad():  # turn off autograd for faster computation and to save memory\n",
    "    predicted_output = self(torch.Tensor(X_test))\n",
    "sum((predicted_output - torch.Tensor(y_test).view(-1,1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: tensor([0.9032, 1.0739, 0.9745, 1.1197, 0.9367, 1.0587, 0.8422, 0.9181, 1.0094,\n",
      "        1.0285, 0.9623, 1.0283, 1.0343, 1.0361, 0.8083, 0.9368, 1.0684, 0.9629,\n",
      "        0.8378, 0.9033, 1.0678, 0.8534, 1.0136, 0.9108, 0.7617, 1.0001, 0.8011,\n",
      "        0.8332, 1.1556, 0.8562, 1.0705, 0.9595, 0.9404, 1.1152, 0.8646, 1.0941,\n",
      "        1.1213, 0.9336, 0.7973, 0.8544, 1.1002, 0.9236, 0.8778, 0.9061, 0.9759,\n",
      "        1.1186, 0.8698, 1.1568, 0.9850, 1.0457])\n",
      "bias: tensor([-1.0789, -1.1505, -0.9150, -1.0567, -1.0803, -1.0560, -0.7792, -1.0625,\n",
      "        -0.8704, -1.0204, -0.7998, -0.9115, -0.8365, -0.7837, -0.7820, -1.0993,\n",
      "        -1.1432, -0.9502, -0.8337, -0.8792, -1.2104, -0.7879, -0.9965, -0.7447,\n",
      "        -0.7826, -1.1842, -0.8015, -0.8918, -1.1742, -0.8663, -1.2020, -0.9834,\n",
      "        -0.9962, -1.0520, -1.0658, -1.0654, -1.1532, -1.0108, -1.0254, -0.9121,\n",
      "        -1.0725, -0.7871, -0.7913, -1.0876, -0.9261, -1.2379, -0.8276, -1.1198,\n",
      "        -0.9347, -1.0529])\n",
      "running_mean: tensor([-0.1449, -0.1433,  0.0378,  0.0114, -0.0700, -0.1677,  0.0849, -0.0127,\n",
      "        -0.1060, -0.1199, -0.1125, -0.1294, -0.0290, -0.0671,  0.1450, -0.1348,\n",
      "        -0.0728, -0.1537, -0.0297, -0.1341, -0.1539, -0.1619, -0.1441,  0.1355,\n",
      "         0.1488,  0.0876, -0.1798,  0.0379,  0.0317,  0.1439, -0.0094, -0.0213,\n",
      "         0.1328,  0.0863,  0.0071, -0.1626,  0.1137, -0.0176, -0.0252, -0.0842,\n",
      "         0.1164, -0.1457,  0.1277, -0.0485, -0.0825, -0.0434, -0.0563, -0.0445,\n",
      "        -0.0167,  0.1645])\n",
      "running_var: tensor([0.8397, 0.8461, 0.6114, 0.8579, 0.9668, 0.6473, 1.8774, 0.9312, 0.6369,\n",
      "        1.0034, 1.0958, 1.3775, 0.6571, 0.8151, 1.0986, 0.9096, 0.6766, 1.1174,\n",
      "        0.4962, 0.7222, 0.7844, 0.9516, 1.0661, 0.6041, 1.6195, 0.6847, 1.6840,\n",
      "        1.4731, 0.7669, 0.5899, 1.0678, 0.8067, 0.6465, 0.7846, 0.7355, 0.7409,\n",
      "        0.6732, 0.6468, 1.0542, 1.2215, 0.8616, 1.1276, 0.7957, 0.6575, 0.6197,\n",
      "        0.8277, 1.2981, 0.7467, 0.9248, 0.7643])\n",
      "num_batches_tracked: 10\n"
     ]
    }
   ],
   "source": [
    "for k,v in self.bn.state_dict().items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class QuickProp(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr)\n",
    "        super(QuickProp, self).__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_p = p.grad\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['prev_delta'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state['prev_update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                prev_delta = state['prev_delta']\n",
    "                prev_update = state['prev_update']\n",
    "\n",
    "                denom = prev_delta - d_p + 1e-10  # Add epsilon to prevent division by zero\n",
    "                update = d_p * prev_update / denom\n",
    "                p.add_(update, alpha=-group['lr'])\n",
    "\n",
    "                # Update state\n",
    "                state['prev_delta'] = d_p.clone()\n",
    "                state['prev_update'] = update.clone()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = QuickProp(self.parameters(), \n",
    "            lr=0.01, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n",
      "tensor([-0.2013,  0.0178,  0.0957, -0.0679, -0.2846,  0.6858,  0.1054,  0.1652,\n",
      "         0.0229, -0.2001, -0.3977,  0.0398, -0.0612], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "self.train()  # turn on dropout\n",
    "\n",
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = self(torch.Tensor(X_train))\n",
    "    loss = criterion(output, torch.Tensor(y_train).view(-1,1))\n",
    "\n",
    "    # Calculates gradient\n",
    "    loss.backward()\n",
    "    print(self.hidden.weight[0])\n",
    "\n",
    "    # Updates weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prev_delta': tensor([[ 5.6412e-02, -1.1343e-01,  1.0727e-01,  6.9792e-02,  1.8458e-01,\n",
      "          1.3629e-01,  1.6069e-01, -5.7747e-02,  1.2965e-01,  1.5276e-01,\n",
      "         -1.1929e-02, -1.7737e-01,  1.7325e-01],\n",
      "        [ 5.8839e-01,  6.9606e-02,  3.9354e-01,  4.4279e-01,  4.3578e-01,\n",
      "         -6.0152e-01,  1.0185e-01,  6.5432e-02,  4.6140e-01,  5.9705e-01,\n",
      "          3.8745e-01, -3.9846e-01,  7.2509e-01],\n",
      "        [ 3.6235e-01, -7.8814e-02,  3.8246e-01,  1.3678e-01,  3.4027e-01,\n",
      "         -5.5767e-01,  2.7598e-01, -7.7951e-02,  3.5379e-01,  4.2411e-01,\n",
      "          2.6482e-01, -3.0242e-01,  5.5051e-01],\n",
      "        [ 6.4720e-01,  6.1097e-01,  3.0900e-01,  6.2558e-01,  3.4412e-01,\n",
      "         -6.3095e-02,  7.4076e-02,  2.1710e-01,  5.2430e-01,  5.8307e-01,\n",
      "          9.3268e-02, -4.1953e-01,  4.9601e-01],\n",
      "        [ 6.7004e-02,  8.6858e-02, -3.6196e-02,  3.7515e-02,  5.1374e-02,\n",
      "          2.4012e-02,  8.8170e-03,  2.4026e-02,  1.2310e-01,  9.4801e-02,\n",
      "          1.0260e-01, -2.2524e-01,  1.3031e-01],\n",
      "        [ 6.1715e-01, -1.8409e-01,  6.9516e-01,  5.9647e-01,  6.8894e-01,\n",
      "         -3.3616e-01,  5.7055e-01, -2.2943e-01,  7.0690e-01,  8.0475e-01,\n",
      "          4.0919e-01, -4.5087e-01,  7.5997e-01],\n",
      "        [ 3.2446e-02,  1.2928e-01,  2.6642e-02,  2.7686e-02,  3.4789e-02,\n",
      "         -6.7254e-02, -7.8670e-03,  9.3543e-02,  3.7807e-02,  6.0147e-02,\n",
      "          6.8283e-03, -4.1579e-02,  9.3660e-02],\n",
      "        [ 3.0251e-02,  4.1906e-01, -3.8741e-02,  2.4448e-02,  3.9498e-02,\n",
      "         -1.3491e-01, -8.6266e-02,  2.7590e-01,  6.0740e-02,  1.5369e-01,\n",
      "         -2.5922e-02, -7.2319e-02,  1.3879e-01],\n",
      "        [ 3.4443e-01, -3.1054e-01,  5.7318e-01,  1.1052e-01,  7.6962e-01,\n",
      "         -2.8191e-01,  5.9308e-01, -4.3947e-01,  5.1280e-01,  6.2140e-01,\n",
      "          1.6358e-01, -5.5829e-01,  5.6017e-01],\n",
      "        [ 3.0146e-01,  4.5048e-01,  1.1137e-01,  9.2423e-02,  1.5160e-01,\n",
      "         -1.9183e-01,  1.5893e-02,  1.1949e-01,  1.9335e-01,  3.0213e-01,\n",
      "         -1.9245e-02, -1.7352e-01,  2.8813e-01],\n",
      "        [ 2.2042e-01,  1.0687e-01,  5.4234e-02, -8.2225e-02,  9.4568e-02,\n",
      "         -2.7499e-01, -1.4021e-03,  1.0423e-01,  1.5106e-01,  1.6774e-01,\n",
      "          9.9398e-02, -2.0940e-01,  2.3606e-01],\n",
      "        [ 8.9655e-02,  7.5662e-02,  6.0178e-02,  1.7491e-01,  1.7717e-01,\n",
      "          8.9257e-02,  8.6807e-02, -3.0525e-02,  9.5057e-02,  1.0854e-01,\n",
      "         -4.4011e-02, -1.4903e-01,  1.5040e-01],\n",
      "        [ 2.3528e-01, -1.1770e-01,  3.2028e-01,  1.3075e-01,  5.1508e-01,\n",
      "          1.7850e-01,  4.8021e-01, -3.4080e-01,  3.4295e-01,  3.8512e-01,\n",
      "          1.2109e-02, -1.1636e-01,  2.8798e-01],\n",
      "        [ 2.3383e-01,  6.1451e-02,  1.3505e-01, -1.0424e-02,  1.8313e-01,\n",
      "         -3.5267e-01,  1.2091e-01,  7.1189e-02,  1.4025e-01,  1.7414e-01,\n",
      "          9.5797e-02, -2.3374e-01,  4.1773e-01],\n",
      "        [ 2.8664e-02,  5.7433e-02,  1.6074e-02,  6.7579e-04,  6.2587e-03,\n",
      "         -5.7209e-02, -5.1984e-03,  3.6677e-02,  1.8889e-02,  4.3236e-02,\n",
      "          1.7069e-02, -4.0013e-02,  5.7390e-02],\n",
      "        [ 2.2752e-01, -3.5659e-01,  3.4521e-01,  6.5534e-01,  5.8173e-01,\n",
      "         -7.0073e-02,  5.2385e-01, -4.1725e-01,  1.6187e-01,  2.0604e-01,\n",
      "         -4.7441e-02, -3.1870e-01,  3.8221e-01],\n",
      "        [ 2.2555e-01, -5.9785e-03,  2.4994e-01,  3.6517e-01,  3.9524e-01,\n",
      "          1.4866e-01,  1.9776e-01, -1.8937e-01,  2.4765e-01,  3.1590e-01,\n",
      "         -1.2582e-01, -1.3258e-01,  2.8194e-01],\n",
      "        [-4.5333e-02, -4.4701e-02,  3.9935e-02,  9.8542e-02,  1.1969e-01,\n",
      "          1.1310e-01,  7.3320e-02,  4.0432e-02,  1.2373e-02,  4.4404e-02,\n",
      "          5.5938e-02, -8.0333e-02,  6.4314e-02],\n",
      "        [ 1.2564e-01, -8.6642e-03,  1.7092e-01,  3.4803e-02,  1.9142e-01,\n",
      "         -1.3177e-01,  1.5189e-01, -1.1481e-01,  1.5748e-01,  1.9369e-01,\n",
      "          8.0886e-02, -1.4068e-01,  2.1093e-01],\n",
      "        [ 3.4225e-01, -3.1626e-03,  3.3579e-01,  5.3643e-02,  3.0487e-01,\n",
      "         -3.0248e-01,  1.7849e-01, -6.2541e-02,  4.0564e-01,  4.6709e-01,\n",
      "          3.3906e-01, -3.0266e-01,  5.0629e-01],\n",
      "        [ 3.4519e-01, -3.3696e-01,  4.3417e-01,  7.0766e-01,  7.2615e-01,\n",
      "          1.9915e-01,  6.2140e-01, -6.2340e-01,  4.6209e-01,  4.4715e-01,\n",
      "          3.3286e-02, -4.3439e-01,  4.5036e-01],\n",
      "        [ 1.4943e-02, -5.6322e-02,  4.0809e-02, -4.0476e-02, -5.4872e-02,\n",
      "         -2.2221e-01, -7.7567e-02,  1.5764e-01, -8.5799e-03,  9.0222e-03,\n",
      "          2.1059e-01,  1.4679e-02,  9.0921e-02],\n",
      "        [ 1.5215e-01,  8.6517e-02,  2.0862e-01,  2.5072e-01,  3.0529e-01,\n",
      "          1.3746e-01,  2.4499e-01, -1.3770e-01,  2.4768e-01,  3.3035e-01,\n",
      "         -5.0794e-02, -1.5244e-01,  1.7063e-01],\n",
      "        [ 9.0210e-02,  1.7019e-02,  6.7082e-02,  1.2799e-02,  5.5285e-02,\n",
      "         -1.1386e-01,  2.7653e-03,  3.5025e-02,  6.6862e-02,  8.4722e-02,\n",
      "          7.2045e-02, -8.7170e-02,  1.3706e-01],\n",
      "        [ 7.0346e-03,  4.6351e-02,  7.6652e-03, -2.0394e-02,  5.0888e-03,\n",
      "         -1.0603e-01, -2.1341e-02,  6.1213e-02, -1.7640e-02,  1.9583e-02,\n",
      "          2.3968e-02,  1.8335e-03,  8.0006e-02],\n",
      "        [ 7.3126e-01, -4.2002e-01,  9.9290e-01,  7.0283e-01,  8.6827e-01,\n",
      "         -5.2326e-01,  7.2728e-01, -4.6212e-01,  7.5244e-01,  9.0654e-01,\n",
      "          5.8450e-01, -6.2070e-01,  9.0853e-01],\n",
      "        [ 2.8827e-02,  6.7870e-02, -5.4072e-03,  9.5515e-02, -1.3492e-02,\n",
      "         -3.1584e-02, -2.3423e-02,  6.5414e-02, -3.4630e-04,  2.5713e-02,\n",
      "          2.5766e-02, -4.7635e-03,  4.2066e-02],\n",
      "        [ 1.7412e-02,  6.0868e-02,  5.4691e-03,  9.8282e-02,  1.7961e-03,\n",
      "         -2.1725e-02,  1.5920e-04,  4.5140e-02,  4.8599e-03,  2.8071e-02,\n",
      "         -3.5252e-02, -4.5998e-02,  4.1081e-02],\n",
      "        [ 9.0072e-01, -4.3915e-01,  9.9393e-01,  8.2545e-01,  1.1317e+00,\n",
      "         -6.3865e-01,  9.6811e-01, -5.4207e-01,  8.4054e-01,  9.5675e-01,\n",
      "          4.7833e-01, -7.2468e-01,  1.3824e+00],\n",
      "        [ 1.9133e-01, -7.2587e-02,  3.2338e-01,  9.5793e-02,  3.9679e-01,\n",
      "         -2.6553e-01,  2.7648e-01, -1.8334e-01,  2.3205e-01,  2.9761e-01,\n",
      "          1.2939e-01, -2.2316e-01,  3.6150e-01],\n",
      "        [ 5.4596e-01,  1.9350e-01,  2.0680e-01,  1.1068e-01,  2.2546e-01,\n",
      "         -4.0430e-01,  1.4682e-02,  1.4994e-01,  4.0743e-01,  4.5154e-01,\n",
      "          4.2848e-01, -3.8370e-01,  6.1172e-01],\n",
      "        [ 2.4875e-01, -2.5658e-01,  2.9047e-01,  2.7306e-01,  4.5683e-01,\n",
      "          5.8583e-02,  3.9286e-01, -3.0962e-01,  2.5319e-01,  2.8404e-01,\n",
      "          3.3042e-02, -3.0523e-01,  3.6561e-01],\n",
      "        [ 4.4277e-01, -2.8010e-01,  5.6146e-01,  3.1578e-01,  6.7524e-01,\n",
      "         -4.5937e-01,  5.5985e-01, -3.3613e-01,  4.2803e-01,  5.1035e-01,\n",
      "          2.7925e-01, -3.8647e-01,  6.7852e-01],\n",
      "        [ 2.2037e-01,  2.2843e-01,  2.9854e-01,  4.4279e-01,  3.4302e-01,\n",
      "          7.8425e-02,  2.5828e-01, -3.0016e-02,  2.1555e-01,  3.1883e-01,\n",
      "         -1.0206e-01, -2.3472e-01,  2.8759e-01],\n",
      "        [ 2.4073e-01, -3.0366e-01,  4.8676e-01,  8.6613e-01,  4.4854e-01,\n",
      "         -4.4408e-01,  2.6995e-01, -1.4360e-01,  3.1210e-01,  4.1899e-01,\n",
      "          3.0105e-01, -2.1549e-01,  5.2792e-01],\n",
      "        [ 5.5736e-01, -3.6334e-01,  6.0924e-01,  6.8173e-01,  8.1337e-01,\n",
      "         -4.1554e-02,  7.2578e-01, -5.0320e-01,  6.0379e-01,  6.4518e-01,\n",
      "          1.8429e-01, -4.4767e-01,  6.1997e-01],\n",
      "        [ 7.8202e-01, -4.1628e-01,  8.1036e-01,  7.0511e-01,  1.1399e+00,\n",
      "          8.6154e-02,  8.9285e-01, -6.1089e-01,  7.5600e-01,  8.9042e-01,\n",
      "          1.9394e-01, -5.0867e-01,  9.3387e-01],\n",
      "        [ 1.0238e-01,  2.3964e-01,  2.4162e-02,  5.0541e-02,  1.7594e-01,\n",
      "         -6.2351e-02,  1.3104e-01,  2.3414e-02,  8.8771e-02,  1.2451e-01,\n",
      "         -1.6304e-01, -1.4640e-01,  2.1375e-01],\n",
      "        [-1.1850e-02,  3.1927e-02,  8.7952e-02,  2.8227e-01,  8.5739e-02,\n",
      "         -8.7288e-02,  1.2773e-01,  1.8735e-02,  8.5349e-03,  6.1829e-02,\n",
      "          5.3035e-02, -2.4146e-02,  1.3670e-01],\n",
      "        [ 5.4470e-02,  1.0761e-01,  3.3497e-02,  1.1029e-01,  4.9917e-02,\n",
      "         -6.2675e-02,  1.7969e-02,  7.3920e-02,  3.7744e-02,  8.7474e-02,\n",
      "          8.4829e-03, -7.3382e-02,  1.0285e-01],\n",
      "        [ 2.5339e-01, -1.3406e-01,  4.1464e-01,  4.1836e-01,  6.5039e-01,\n",
      "          1.1764e-01,  5.8443e-01, -3.7341e-01,  3.4865e-01,  3.9029e-01,\n",
      "         -1.9614e-01, -3.2049e-01,  4.7712e-01],\n",
      "        [ 2.4568e-02,  3.9232e-02, -3.1071e-02,  1.8729e-03, -6.7643e-03,\n",
      "         -3.6685e-02, -4.7458e-02,  6.2845e-02,  1.7453e-03,  4.2754e-05,\n",
      "          3.0068e-03, -2.0676e-02,  4.0386e-02],\n",
      "        [ 3.9766e-02, -1.5398e-02,  6.6868e-03, -9.7077e-03,  4.2489e-02,\n",
      "         -7.4607e-02,  1.2384e-02,  3.0154e-02,  2.1809e-02,  2.8520e-02,\n",
      "          9.0857e-02, -7.9733e-02,  1.0612e-01],\n",
      "        [ 3.5198e-01, -2.4263e-01,  5.6276e-01,  4.9292e-01,  7.7203e-01,\n",
      "         -1.5511e-01,  6.6234e-01, -5.2901e-01,  5.1922e-01,  6.1354e-01,\n",
      "          9.5527e-02, -3.4961e-01,  4.5951e-01],\n",
      "        [ 4.8582e-01, -1.4074e-01,  3.8018e-01,  2.5655e-01,  4.7764e-01,\n",
      "         -4.1327e-01,  2.6890e-01, -1.3050e-01,  4.1784e-01,  5.0494e-01,\n",
      "          2.7011e-01, -4.1349e-01,  6.8323e-01],\n",
      "        [ 9.4913e-01, -3.0186e-01,  1.1175e+00,  7.9652e-01,  1.1777e+00,\n",
      "         -5.8206e-01,  8.0422e-01, -3.3939e-01,  9.1050e-01,  1.1190e+00,\n",
      "          6.2127e-01, -7.1564e-01,  1.3286e+00],\n",
      "        [ 5.9126e-02,  6.9942e-02,  2.9148e-02,  9.8659e-02,  9.5125e-02,\n",
      "          3.6264e-02,  8.5906e-02,  1.2219e-02,  5.4369e-02,  6.5795e-02,\n",
      "         -1.9069e-02, -3.9392e-02,  9.4428e-02],\n",
      "        [ 5.7660e-01, -3.7871e-01,  8.8443e-01,  8.4188e-01,  1.0804e+00,\n",
      "         -1.1555e-01,  9.2742e-01, -8.2893e-01,  6.4503e-01,  8.7980e-01,\n",
      "         -4.0251e-02, -4.3869e-01,  6.3974e-01],\n",
      "        [ 1.2154e-01, -5.6644e-02,  2.5101e-01,  4.4198e-01,  3.5128e-01,\n",
      "         -1.7854e-03,  2.6158e-01, -2.2467e-01,  1.9973e-01,  2.4508e-01,\n",
      "          7.8820e-02, -1.3961e-01,  2.3241e-01],\n",
      "        [ 4.9251e-01,  2.7969e-01,  1.8785e-01,  3.4281e-01,  3.3123e-01,\n",
      "         -6.2429e-03,  3.7139e-02,  8.8500e-02,  2.0697e-01,  2.9929e-01,\n",
      "         -1.3356e-01, -2.9030e-01,  4.6719e-01]]), 'prev_update': tensor([[0., -0., 0., -0., -0., 0., -0., -0., -0., -0., 0., 0., 0.],\n",
      "        [0., 0., 0., -0., -0., 0., -0., 0., -0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., 0., 0., 0., -0., -0., 0., 0., 0., 0., -0.],\n",
      "        [0., -0., 0., 0., -0., 0., 0., -0., 0., 0., -0., 0., -0.],\n",
      "        [0., -0., -0., 0., -0., -0., -0., 0., -0., -0., -0., 0., -0.],\n",
      "        [-0., 0., 0., -0., -0., -0., 0., 0., 0., -0., 0., -0., 0.],\n",
      "        [-0., -0., 0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., 0., -0., -0., 0., -0., -0., -0., 0., 0., -0., -0.],\n",
      "        [-0., -0., 0., -0., -0., -0., -0., 0., 0., 0., 0., -0., -0.],\n",
      "        [0., -0., 0., 0., -0., -0., 0., 0., 0., -0., 0., 0., -0.],\n",
      "        [0., 0., -0., 0., 0., -0., -0., 0., 0., -0., 0., -0., 0.],\n",
      "        [0., 0., -0., -0., 0., -0., -0., 0., -0., -0., 0., -0., 0.],\n",
      "        [-0., -0., 0., -0., 0., 0., 0., -0., -0., -0., -0., 0., 0.],\n",
      "        [-0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0., -0., 0.],\n",
      "        [-0., -0., -0., 0., -0., -0., 0., 0., 0., -0., 0., 0., 0.],\n",
      "        [-0., 0., -0., 0., -0., 0., 0., 0., -0., -0., 0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0.],\n",
      "        [-0., -0., 0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0.],\n",
      "        [-0., -0., -0., 0., 0., -0., -0., -0., -0., 0., -0., 0., -0.],\n",
      "        [0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0.],\n",
      "        [-0., 0., -0., -0., -0., 0., 0., -0., 0., 0., -0., 0., 0.],\n",
      "        [-0., -0., 0., -0., -0., 0., -0., -0., -0., -0., -0., 0., -0.],\n",
      "        [0., 0., -0., 0., 0., -0., 0., 0., -0., -0., 0., -0., 0.],\n",
      "        [-0., 0., -0., -0., -0., -0., -0., 0., 0., -0., -0., 0., -0.],\n",
      "        [-0., -0., -0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0.],\n",
      "        [0., 0., 0., -0., -0., -0., -0., 0., -0., -0., -0., -0., 0.],\n",
      "        [0., -0., 0., -0., 0., -0., -0., -0., 0., -0., 0., -0., -0.],\n",
      "        [-0., -0., -0., -0., 0., -0., 0., 0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0.],\n",
      "        [-0., 0., 0., 0., 0., 0., 0., -0., -0., -0., 0., 0., -0.],\n",
      "        [-0., 0., 0., 0., 0., 0., -0., -0., 0., 0., 0., -0., -0.],\n",
      "        [0., -0., -0., -0., 0., -0., -0., 0., -0., -0., -0., 0., -0.],\n",
      "        [0., -0., -0., -0., -0., 0., 0., 0., -0., 0., 0., 0., 0.],\n",
      "        [-0., 0., -0., 0., 0., -0., -0., -0., 0., -0., -0., 0., -0.],\n",
      "        [-0., 0., 0., -0., -0., -0., 0., -0., -0., 0., 0., 0., -0.],\n",
      "        [-0., -0., -0., -0., 0., -0., -0., 0., -0., 0., 0., -0., -0.],\n",
      "        [-0., 0., -0., -0., 0., 0., -0., 0., 0., -0., -0., -0., -0.],\n",
      "        [0., 0., -0., -0., -0., 0., 0., 0., 0., -0., 0., 0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., -0., 0., 0., -0., -0., 0., 0.],\n",
      "        [-0., 0., 0., 0., 0., 0., -0., -0., 0., 0., -0., -0., -0.],\n",
      "        [0., -0., 0., 0., 0., 0., -0., -0., 0., -0., 0., -0., 0.],\n",
      "        [0., 0., -0., -0., 0., 0., 0., 0., 0., -0., 0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., -0., 0., 0., -0., -0., 0., -0., 0.],\n",
      "        [-0., 0., -0., -0., -0., 0., 0., -0., -0., -0., -0., 0., -0.],\n",
      "        [0., -0., -0., 0., 0., -0., 0., 0., -0., 0., 0., -0., 0.],\n",
      "        [-0., -0., 0., 0., 0., 0., -0., 0., 0., 0., -0., 0., -0.],\n",
      "        [0., -0., -0., 0., -0., 0., -0., 0., 0., 0., 0., -0., -0.],\n",
      "        [0., -0., 0., -0., 0., 0., 0., 0., -0., -0., 0., 0., 0.],\n",
      "        [0., 0., 0., -0., 0., -0., 0., -0., 0., -0., 0., 0., 0.],\n",
      "        [-0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., -0., 0.]])}\n",
      "{'prev_delta': tensor([-2.2817e-08,  2.1234e-07, -3.1665e-08, -4.0233e-07,  4.1910e-08,\n",
      "        -1.7509e-07, -1.0245e-08, -1.6764e-08,  2.4214e-08, -8.3819e-09,\n",
      "        -6.6822e-08,  1.4901e-08,  6.9849e-08,  1.3039e-07, -5.3551e-09,\n",
      "         8.1956e-08,  2.2352e-07,  1.7136e-07,  1.8394e-08,  3.7253e-09,\n",
      "         2.1048e-07,  1.6997e-08,  1.1362e-07, -1.4668e-08,  7.8522e-08,\n",
      "        -2.6822e-07,  4.4703e-08, -1.0245e-08, -4.0233e-07,  1.6764e-08,\n",
      "        -3.7253e-09, -1.1548e-07,  1.8626e-08,  2.0489e-08,  3.7253e-08,\n",
      "        -9.6858e-08,  6.8801e-08,  1.7881e-07,  1.1548e-07, -5.9605e-08,\n",
      "         1.8626e-08, -2.7940e-09, -2.7940e-08, -5.2154e-08,  2.9802e-08,\n",
      "         1.7136e-07,  8.1956e-08, -1.1921e-07, -9.6858e-08,  9.6858e-08]), 'prev_update': tensor([0., 0., 0., -0., -0., 0., 0., -0., 0., -0., -0., 0., 0., 0., -0., -0., 0., 0., -0., 0., 0., -0., 0., 0.,\n",
      "        -0., -0., 0., 0., -0., 0., -0., -0., 0., -0., -0., -0., 0., -0., -0., -0., 0., -0., -0., -0., 0., -0., -0., -0.,\n",
      "        -0., -0.])}\n",
      "{'prev_delta': tensor([-1.5606, -0.7189, -0.1311, -2.1657, -2.1743, -0.7641, -0.7896, -1.8069,\n",
      "        -0.5079, -1.5382,  0.0280, -1.7771, -1.0195, -0.0197, -0.3411, -1.8927,\n",
      "        -1.8367, -1.8733, -0.0930, -0.2746, -2.2826, -1.4037, -1.6179, -0.0103,\n",
      "        -1.0359, -0.9703, -1.2527, -1.3535, -1.2638, -0.3358, -0.5847, -1.2190,\n",
      "        -0.4375, -1.7068, -1.4789, -1.4895, -1.8576, -1.1897, -1.3322, -1.0599,\n",
      "        -1.7263,  0.0928,  0.1449, -1.3467, -0.2799, -1.5433, -1.0099, -1.9164,\n",
      "        -1.6419, -1.5624]), 'prev_update': tensor([-0., 0., -0., -0., -0., 0., 0., 0., -0., -0., 0., 0., -0., -0., -0., -0., 0., -0., 0., -0., -0., 0., -0., 0.,\n",
      "        0., -0., 0., -0., -0., 0., -0., 0., -0., 0., 0., 0., 0., 0., 0., -0., -0., 0., -0., 0., -0., 0., -0., -0.,\n",
      "        -0., 0.])}\n",
      "{'prev_delta': tensor([-3.1715, -3.2968, -1.5027, -3.9816, -3.7215, -2.5022, -1.9900, -3.2973,\n",
      "        -1.4837, -3.1034, -1.2732, -3.5299, -2.3071, -1.1521, -0.8140, -4.0509,\n",
      "        -3.2399, -3.4525, -0.4899, -1.3416, -4.3798, -2.6279, -3.2453, -0.4267,\n",
      "        -2.1577, -3.2694, -2.4936, -2.5425, -3.9488, -1.0516, -3.7946, -2.9741,\n",
      "        -1.9814, -3.4595, -3.3563, -3.4190, -3.9868, -2.2589, -2.7005, -2.2700,\n",
      "        -3.6313, -0.5989, -0.8000, -2.9500, -1.7924, -4.2051, -2.3103, -4.0948,\n",
      "        -3.3746, -3.2092]), 'prev_update': tensor([-0., -0., 0., 0., -0., -0., -0., 0., -0., 0., 0., 0., 0., -0., -0., -0., 0., 0., 0., 0., 0., 0., -0., -0.,\n",
      "        0., -0., -0., 0., 0., -0., -0., 0., 0., -0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0., -0., -0., -0., 0.,\n",
      "        -0., -0.])}\n",
      "{'prev_delta': tensor([[-5.9071, -4.5817, -5.2795, -6.4791, -6.1225, -5.8975, -6.9576, -6.3497,\n",
      "         -6.8378, -6.3870, -5.4172, -7.1104, -6.8506, -5.6054, -6.7301, -5.8219,\n",
      "         -6.1580, -6.6759, -6.1650, -5.5465, -5.8694, -7.7236, -6.3104, -5.5455,\n",
      "         -7.1487, -4.6251, -7.0166, -6.8992, -5.3700, -5.9406, -4.2085, -6.2607,\n",
      "         -5.4705, -6.7640, -6.0296, -5.9987, -6.0708, -6.5569, -5.9121, -6.7224,\n",
      "         -6.2317, -5.0867, -5.0403, -5.9400, -5.2231, -5.4739, -7.2077, -6.0281,\n",
      "         -6.8696, -6.0022]]), 'prev_update': tensor([[-0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., -0., -0., -0., -0., 0., -0., -0., -0., -0., 0., -0., 0., 0.,\n",
      "         0., -0., -0., -0., -0., 0., 0., 0., -0., 0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0., -0.,\n",
      "         -0., -0.]])}\n",
      "{'prev_delta': tensor([-18.3624]), 'prev_update': tensor([-0.])}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for k,v in optimizer.state.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c2396ee008e4910c299ae7134fa9bf09084771bab5830620999247b4a514b46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
